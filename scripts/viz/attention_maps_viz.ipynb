{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9267eedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "from typing import Any, Tuple, List, Optional\n",
    "from hydra import compose, initialize_config_dir\n",
    "from hydra.core.global_hydra import GlobalHydra\n",
    "from hydra.utils import instantiate\n",
    "\n",
    "# --- USER CONFIGURATION START ---\n",
    "# UPDATE THESE PATHS BEFORE RUNNING\n",
    "REPO_ROOT = Path(\"/path/to/your/drivor_repo\")  # e.g., /home/user/workspace/drivor\n",
    "DATASET_ROOT = Path(\"/path/to/datasets\")       # e.g., /datasets_local/navsim_workspace\n",
    "\n",
    "# Set specific dataset paths\n",
    "os.environ[\"NUPLAN_MAPS_ROOT\"] = str(DATASET_ROOT / \"dataset/maps\")\n",
    "os.environ[\"NUPLAN_MAP_VERSION\"] = \"nuplan-maps-v1.0\"\n",
    "os.environ[\"OPENSCENE_DATA_ROOT\"] = str(DATASET_ROOT / \"dataset/openscene-v1.1\")\n",
    "os.environ[\"NAVSIM_EXP_ROOT\"] = str(DATASET_ROOT / \"exp\")\n",
    "os.environ[\"NAVSIM_DEVKIT_ROOT\"] = str(REPO_ROOT)\n",
    "\n",
    "# Model Checkpoint\n",
    "CHECKPOINT_PATH = REPO_ROOT / \"your ckpt\"\n",
    "EXPERIMENT_NAME = \"attention_map_viz\"\n",
    "AGENT_NAME = \"drivoR\"\n",
    "# --- USER CONFIGURATION END ---\n",
    "\n",
    "# Ensure repo is in python path\n",
    "if str(REPO_ROOT) not in sys.path:\n",
    "    sys.path.append(str(REPO_ROOT))\n",
    "\n",
    "# Navsim Imports (must happen after sys.path update)\n",
    "import navsim.common.dataclasses as dc\n",
    "from navsim.agents.abstract_agent import AbstractAgent\n",
    "from navsim.common.dataloader import SceneLoader, SceneFilter, SensorConfig, MetricCacheLoader\n",
    "from navsim.visualization.config import TRAJECTORY_CONFIG, CAMERAS_PLOT_CONFIG\n",
    "from navsim.visualization.bev import add_configured_bev_on_ax, add_trajectory_to_bev_ax\n",
    "from navsim.visualization.plots import configure_bev_ax\n",
    "from navsim.common.dataclasses import Trajectory\n",
    "\n",
    "# Sanity Check\n",
    "assert Path(os.environ[\"NUPLAN_MAPS_ROOT\"]).exists(), f\"Maps root not found: {os.environ['NUPLAN_MAPS_ROOT']}\"\n",
    "print(\"Environment configured successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a562958",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_agent_and_scenes(\n",
    "    repo_path: Path, \n",
    "    checkpoint_path: Path, \n",
    "    agent_name: str, \n",
    "    experiment_name: str\n",
    "):\n",
    "    \"\"\"\n",
    "    Loads the Agent, and two SceneLoaders (one for inference, one for visualization).\n",
    "    \"\"\"\n",
    "    config_dir = repo_path / \"navsim/planning/script/config/training\"\n",
    "    agent_config_dir = repo_path / \"navsim/planning/script/config/common/agent\"\n",
    "    \n",
    "    # Overrides for the general experiment\n",
    "    overrides = [\n",
    "        \"train_test_split=navtrain\",\n",
    "        f\"experiment_name={experiment_name}\",\n",
    "    ]\n",
    "\n",
    "    # Overrides strictly for the Agent\n",
    "    agent_overrides = [\n",
    "        f\"checkpoint_path={str(checkpoint_path)}\",\n",
    "        \"config.shared_refiner=false\",\n",
    "        \"lr_args=null\",\n",
    "        \"scheduler_args=null\",\n",
    "        \"progress_bar=false\",\n",
    "        \"config.refiner_ls_values=0.0\",\n",
    "        \"config.image_backbone.focus_front_cam=false\",\n",
    "        \"config.one_token_per_traj=true\",\n",
    "        \"config.refiner_num_heads=1\",\n",
    "        \"config.tf_d_model=256\",\n",
    "        \"config.tf_d_ffn=1024\",\n",
    "        \"config.area_pred=false\",\n",
    "        \"config.agent_pred=false\",\n",
    "        \"config.ref_num=4\",\n",
    "        \"config.noc=1\", \"config.dac=1\", \"config.ddc=0\",\n",
    "        \"config.ttc=5\", \"config.ep=5\", \"config.comfort=2\",\n",
    "        \"loss.prev_weight=0.0\",\n",
    "        \"batch_size=null\",\n",
    "    ]\n",
    "\n",
    "    # clear hydra instance if running in notebook\n",
    "    if GlobalHydra.instance().is_initialized():\n",
    "        GlobalHydra.instance().clear()\n",
    "\n",
    "    # 1. Initialize Agent\n",
    "    with initialize_config_dir(version_base=None, config_dir=str(agent_config_dir)):\n",
    "        agent_cfg = compose(config_name=agent_name, overrides=agent_overrides)\n",
    "    \n",
    "    agent: AbstractAgent = instantiate(agent_cfg)\n",
    "    agent.initialize()\n",
    "    \n",
    "    # Optional: Load metric cache if needed\n",
    "    cache_path = Path(os.environ[\"NAVSIM_EXP_ROOT\"]) / \"train_metric_cache\"\n",
    "    if cache_path.exists():\n",
    "        agent.test_metric_cache_paths = MetricCacheLoader(cache_path).metric_cache_paths\n",
    "    \n",
    "    agent.b2d = False\n",
    "    agent.ray = False\n",
    "\n",
    "    # 2. Initialize Scene Loaders\n",
    "    with initialize_config_dir(version_base=None, config_dir=str(config_dir)):\n",
    "        cfg = compose(config_name=\"default_training\", overrides=overrides)\n",
    "\n",
    "    scene_filter: SceneFilter = instantiate(cfg.train_test_split.scene_filter)\n",
    "    scene_filter.log_names = cfg.val_logs\n",
    "\n",
    "    # Loader for model inference\n",
    "    inference_loader = SceneLoader(\n",
    "        sensor_blobs_path=Path(cfg.sensor_blobs_path),\n",
    "        data_path=Path(cfg.navsim_log_path),\n",
    "        scene_filter=scene_filter,\n",
    "        sensor_config=agent.get_sensor_config()\n",
    "    )\n",
    "    \n",
    "    # Loader for visualization (includes images)\n",
    "    viz_sensor_config = SensorConfig(\n",
    "        cam_f0=[3], cam_l0=[3], cam_l1=[3], cam_l2=[3],\n",
    "        cam_r0=[3], cam_r1=[3], cam_r2=[3], cam_b0=[3],\n",
    "        lidar_pc=False,\n",
    "    )\n",
    "    viz_loader = SceneLoader(\n",
    "        sensor_blobs_path=Path(cfg.sensor_blobs_path),\n",
    "        data_path=Path(cfg.navsim_log_path),\n",
    "        scene_filter=scene_filter,\n",
    "        sensor_config=viz_sensor_config\n",
    "    )\n",
    "    \n",
    "    return agent, inference_loader, viz_loader\n",
    "\n",
    "print(\"Loading model...\")\n",
    "agent, inference_loader, viz_loader = load_agent_and_scenes(\n",
    "    REPO_ROOT, CHECKPOINT_PATH, AGENT_NAME, EXPERIMENT_NAME\n",
    ")\n",
    "print(\"Model loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d50781c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_heads(A):\n",
    "    \"\"\" (B,H,N,N) -> (B,N,N) \"\"\"\n",
    "    A = A.mean(1)\n",
    "    return A / (A.sum(-1, keepdim=True) + 1e-9)\n",
    "\n",
    "@torch.no_grad()\n",
    "def reg_to_patch_last_layer(attn_last, reg_idx, patch_idx, H_p, W_p):\n",
    "    \"\"\"Simple extraction from the final layer.\"\"\"\n",
    "    A = avg_heads(attn_last)                  # (B,N,N)\n",
    "    V = A[:, reg_idx][:, :, patch_idx]        # (B,R,P)\n",
    "    return V.view(V.shape[0], V.shape[1], H_p, W_p)\n",
    "\n",
    "@torch.no_grad()\n",
    "def reg_to_patch_shallow_rollout(attn_list, reg_idx, patch_idx, H_p, W_p, last_k=3, alpha=0.9):\n",
    "    \"\"\"\n",
    "    Rollout attention over the last k layers to capture deeper dependencies.\n",
    "    \"\"\"\n",
    "    mats = []\n",
    "    for A in attn_list[-last_k:]:\n",
    "        M = avg_heads(A).clone()\n",
    "        I = torch.eye(M.size(-1), device=M.device, dtype=M.dtype).expand_as(M)\n",
    "        M = alpha * M + (1 - alpha) * I\n",
    "        mats.append(M)\n",
    "    \n",
    "    R = mats[0]\n",
    "    for M in mats[1:]:\n",
    "        R = R @ M\n",
    "        \n",
    "    R = R / (R.sum(-1, keepdim=True) + 1e-9)\n",
    "    V = R[:, reg_idx][:, :, patch_idx]\n",
    "    return V.view(V.shape[0], V.shape[1], H_p, W_p)\n",
    "\n",
    "@torch.no_grad()\n",
    "def reg_to_patch_low_entropy_head(attn_last, reg_idx, patch_idx, H_p, W_p):\n",
    "    \"\"\"\n",
    "    Selects the attention head with the lowest entropy for sharper visualizations.\n",
    "    \"\"\"\n",
    "    B, H, N, _ = attn_last.shape\n",
    "    # Slice reg->patch per head: (B,H,R,P)\n",
    "    R2P = attn_last[:, :, reg_idx][:, :, :, patch_idx]\n",
    "    R2P = R2P / (R2P.sum(-1, keepdim=True) + 1e-9)\n",
    "    \n",
    "    # Calculate entropy\n",
    "    ent = -(R2P.clamp_min(1e-12) * R2P.clamp_min(1e-12).log()).sum(-1) # (B,H,R)\n",
    "\n",
    "    # Pick best head per (B,R)\n",
    "    best_h = ent.argmin(1) # (B,R)\n",
    "    out = torch.zeros((B, len(reg_idx), H_p, W_p), device=attn_last.device, dtype=attn_last.dtype)\n",
    "    \n",
    "    for b in range(B):\n",
    "        # Gather per-reg head: (R,P)\n",
    "        sel = R2P[b, best_h[b], torch.arange(len(reg_idx), device=attn_last.device)]\n",
    "        out[b] = sel.view(len(reg_idx), H_p, W_p)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f240e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_trajectories_by_camera(proposals, A_tr2reg, ego_yaw=0.0, title=\"Trajectory Attention\"):\n",
    "    \"\"\"Plots decoded trajectories colored by their dominant camera attention.\"\"\"\n",
    "    # 1. Compute per-camera attention strength\n",
    "    if A_tr2reg.ndim == 2:\n",
    "        A_tr2reg_cams = A_tr2reg.view(64, 4, 16) # (64 regs total -> 4 cams * 16 regs)\n",
    "    else:\n",
    "        A_tr2reg_cams = A_tr2reg\n",
    "\n",
    "    cam_strength = A_tr2reg_cams.sum(-1)\n",
    "    cam_strength = cam_strength / (cam_strength.sum(-1, keepdim=True) + 1e-9)\n",
    "    dominant_cam = cam_strength.argmax(dim=-1).cpu().numpy()\n",
    "    cam_confidence = cam_strength.max(-1).values.cpu().numpy()\n",
    "\n",
    "    # 2. Rotate coordinates\n",
    "    rot = np.array([\n",
    "        [np.cos(ego_yaw), -np.sin(ego_yaw)],\n",
    "        [np.sin(ego_yaw),  np.cos(ego_yaw)],\n",
    "    ])\n",
    "    xy = proposals[..., :2].cpu().numpy() @ rot.T\n",
    "\n",
    "    # 3. Setup Plot\n",
    "    camera_labels = [\"Front\", \"Back\", \"Left\", \"Right\"]\n",
    "    camera_colors = [\"#E41A1C\", \"#377EB8\", \"#4DAF4A\", \"#984EA3\"] \n",
    "    colors = np.array(camera_colors)[dominant_cam]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    \n",
    "    for i in range(xy.shape[0]):\n",
    "        x, y = xy[i, :, 0], xy[i, :, 1]\n",
    "        alpha = 0.4 + 0.6 * cam_confidence[i]\n",
    "        ax.plot(x, y, color=colors[i], alpha=alpha, lw=2, zorder=2)\n",
    "        ax.scatter(x[-1], y[-1], color=colors[i], s=10, zorder=3)\n",
    "\n",
    "    ego_shape = np.array([[-1.0, -0.5], [1.0, -0.5], [1.0, 0.5], [-1.0, 0.5]]) @ rot.T\n",
    "    ax.add_patch(plt.Polygon(ego_shape, closed=True, color=\"gray\", alpha=0.5, zorder=4))\n",
    "\n",
    "    handles = [plt.Line2D([0], [0], color=c, lw=3, label=l) for c, l in zip(camera_colors, camera_labels)]\n",
    "    ax.legend(handles=handles, title=\"Dominant Camera\", loc=\"upper right\")\n",
    "    \n",
    "    pad = 5.0\n",
    "    ax.set_xlim(xy[..., 0].min() - pad, xy[..., 0].max() + pad)\n",
    "    ax.set_ylim(xy[..., 1].min() - pad, xy[..., 1].max() + pad)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def visualize_attention_maps(reg_maps, cams, R=16, H_img=672, W_img=1148):\n",
    "    \"\"\"Visualizes attention maps overlaid on camera images.\"\"\"\n",
    "    PATCH_SIZE = 14\n",
    "    H_p, W_p = reg_maps.shape[-2:]\n",
    "    \n",
    "    for b, (title, cam_img) in enumerate(cams.items()):\n",
    "        maps_b = reg_maps[b] # (R, H_p, W_p)\n",
    "\n",
    "        if torch.is_tensor(cam_img):\n",
    "            cam_img = cam_img.detach().cpu().permute(1, 2, 0).numpy()\n",
    "        if cam_img.max() > 1.1:\n",
    "            cam_img = cam_img / 255.0\n",
    "            \n",
    "        img_resized = cv2.resize(cam_img, (W_img, H_img), interpolation=cv2.INTER_AREA)\n",
    "        H_crop, W_crop = H_p * PATCH_SIZE, W_p * PATCH_SIZE\n",
    "        img_cropped = img_resized[:H_crop, :W_crop]\n",
    "\n",
    "        plt.figure(figsize=(12, 12))\n",
    "        for i in range(min(R, 16)): \n",
    "            plt.subplot(4, 4, i + 1)\n",
    "            hm = maps_b[i].detach().cpu().numpy()\n",
    "            hm = (hm - hm.min()) / (np.ptp(hm) + 1e-9)\n",
    "            hm_resized = cv2.resize(hm, (W_crop, H_crop), interpolation=cv2.INTER_NEAREST)\n",
    "            \n",
    "            plt.imshow(np.clip(img_cropped, 0, 1))\n",
    "            plt.imshow(hm_resized, alpha=0.5, cmap='magma')\n",
    "            plt.title(f\"{title} Reg {i}\")\n",
    "            plt.axis('off')\n",
    "        \n",
    "        plt.suptitle(f\"{title.upper()} — Register Attention Maps\", fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "def plot_patch_energy(reg_maps, cams):\n",
    "    \"\"\"Plots the mean attention energy over patches (Universal Attractors).\"\"\"\n",
    "    for b, (title, _) in enumerate(cams.items()):\n",
    "        patch_energy = reg_maps[b].mean(0)  # (H_p, W_p)\n",
    "        plt.figure(figsize=(6, 5))\n",
    "        plt.imshow(patch_energy.cpu(), cmap='hot')\n",
    "        plt.title(f\"{title.upper()} — Patch Attention Energy\")\n",
    "        plt.colorbar(label=\"Mean attention\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "def plot_register_similarity(reg_maps, cams):\n",
    "    \"\"\"Plots the cosine similarity between registers within a specific view.\"\"\"\n",
    "    for b, (title, _) in enumerate(cams.items()):\n",
    "        rf = reg_maps[b].flatten(1)     # (R, P)\n",
    "        rf = rf / (rf.norm(dim=-1, keepdim=True) + 1e-9)\n",
    "        sim = rf @ rf.T                 # (R, R)\n",
    "        \n",
    "        plt.figure(figsize=(4.5, 4))\n",
    "        plt.imshow(sim.cpu(), cmap='viridis', vmin=0, vmax=1)\n",
    "        plt.title(f\"{title.upper()} — Reg↔Reg Cosine\")\n",
    "        plt.colorbar(label=\"cosine sim\")\n",
    "        plt.xlabel(\"Reg j\")\n",
    "        plt.ylabel(\"Reg i\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        print(f\"{title} mean inter-register similarity: {sim.triu(1).mean().item():.4f}\")\n",
    "\n",
    "def plot_layer_divergence(attentions_list, cams, R=16):\n",
    "    \"\"\"Plots how register specialization (cosine similarity) changes across layers.\"\"\"\n",
    "    for b, (title, _) in enumerate(cams.items()):\n",
    "        divergence = []\n",
    "        for l, A in enumerate(attentions_list):  # A: (B,H,N,N)\n",
    "            # Select camera b, average heads, select first R registers\n",
    "            regs = A.mean(1)[b, :R, :] # (R,N)\n",
    "            sim = F.cosine_similarity(regs[:, None, :], regs[None, :, :], dim=-1)\n",
    "            mean_offdiag = sim.triu(1).mean().item()\n",
    "            divergence.append(mean_offdiag)\n",
    "\n",
    "        plt.figure(figsize=(5, 3))\n",
    "        plt.plot(divergence, marker='o')\n",
    "        plt.xlabel(\"Layer index\")\n",
    "        plt.ylabel(\"Mean inter-register cosine sim\")\n",
    "        plt.title(f\"{title.upper()} — Register Specialization vs Depth\")\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a196330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Select a Scene\n",
    "# You can pick a random one, or a specific token\n",
    "token = np.random.choice(inference_loader.tokens)\n",
    "# token = \"02abc6b6508f5516\" # specific example\n",
    "print(f\"Visualizing Token: {token}\")\n",
    "\n",
    "scene = viz_loader.get_scene_from_token(token)\n",
    "agent_input = inference_loader.get_agent_input_from_token(token)\n",
    "\n",
    "# 2. Run Inference\n",
    "agent.eval()\n",
    "features = {}\n",
    "for builder in agent.get_feature_builders():\n",
    "    features.update(builder.compute_features(agent_input))\n",
    "features = {k: v.unsqueeze(0) for k, v in features.items()} # Add batch dim\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions = agent.forward(features)\n",
    "\n",
    "# 3. Extract Outputs\n",
    "img_attentions = predictions[\"image_backbone_attentions\"]\n",
    "traj_attentions = predictions[\"trajectory_attentions\"][1]\n",
    "proposals = predictions[\"proposals\"][0] # (64, num_poses, 3)\n",
    "\n",
    "# 4. Process Attention Maps (Config)\n",
    "# Constants based on architecture\n",
    "R = 16 \n",
    "N_total = img_attentions[0].shape[-1]\n",
    "prefix = 21 # 16 regs + 5 specials\n",
    "H_p, W_p = 48, 82\n",
    "\n",
    "reg_idx = torch.arange(0, R, device=img_attentions[0].device)\n",
    "patch_idx = torch.arange(prefix, N_total, device=img_attentions[0].device)\n",
    "\n",
    "# 5. Generate Attention Maps (Choose your aggregation method)\n",
    "# Options: \"last\", \"shallow\", \"besthead\"\n",
    "aggregation = \"besthead\" \n",
    "\n",
    "if aggregation == \"last\":\n",
    "    reg_maps = reg_to_patch_last_layer(img_attentions[-1], reg_idx, patch_idx, H_p, W_p)\n",
    "elif aggregation == \"shallow\":\n",
    "    reg_maps = reg_to_patch_shallow_rollout(img_attentions, reg_idx, patch_idx, H_p, W_p)\n",
    "else:\n",
    "    reg_maps = reg_to_patch_low_entropy_head(img_attentions[-1], reg_idx, patch_idx, H_p, W_p)\n",
    "\n",
    "# 6. Visualize\n",
    "cams = {\n",
    "    \"cam_f0\": scene.frames[3].cameras.cam_f0.image,\n",
    "    \"cam_b0\": scene.frames[3].cameras.cam_b0.image,\n",
    "    \"cam_l0\": scene.frames[3].cameras.cam_l0.image,\n",
    "    \"cam_r0\": scene.frames[3].cameras.cam_r0.image,\n",
    "}\n",
    "\n",
    "print(f\"Visualizing Method: {aggregation}\")\n",
    "visualize_attention_maps(reg_maps, cams)\n",
    "\n",
    "print(\"Visualizing Trajectory-Camera Alignment\")\n",
    "A_tr2reg = traj_attentions[-1][0]\n",
    "plot_trajectories_by_camera(proposals, A_tr2reg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dynamo_iPad_fork",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
